npj |digita lmedicine Article
Published in partnership with Seoul National University Bundang Hospital
https://doi.org/10.1038/s41746-024-01377-1
Clinical entity augmented retrieval for
clinical information extraction
Check for updates
Ivan Lopez1,2,14, Akshay Swaminathan1,2,14, Karthik Vedula3, Sanjana Narayanan4,
Fateme Nateghi Haredasht4,S t e p h e nP .M a5, April S. Liang6,S t e v e nT a t e7,M a n o jM a d d a l i2,8,
Robert Joseph Gallo9,10, Nigam H. Shah4,11,12&J o n a t h a nH .C h e n2,4,5,12,13
Large language models (LLMs) with retrieval-augmented generation (RAG) have improved information
extraction over previous methods, yet their reliance on embeddings often leads to inef ﬁcient retrieval.
We introduce CLinical Entity Augmented Retrieval (CLEAR), a RAG pipeline that retrieves informationusing entities. We compared CLEAR to embedding RAG and full-note approaches for extracting 18variables using six LLMs across 20,000 clinical notes. Average F1 scores were 0.90, 0.86, and 0.79;inference times were 4.95, 17.41, and 20.08 s per note; average model queries were 1.68, 4.94, and4.18 per note; and average input tokens were 1.1k, 3.8k, and 6.1k per note for CLEAR, embeddingRAG, and full-note approaches, respectively. In conclusion, CLEAR utilizes clinical entities for
information retrieval and achieves >70% reduction in token usage and inference time with improved
performance compared to modern methods.
Free-text notes in electronic health r ecords (EHRs) are rich with data not
found within structured ﬁelds, like symptoms, diagnoses, disease course,
social determinants of health, famil y history, and patient perspectives1,2.T h e
ability to process this data unlocks va rious important research and quality
improvement use cases, including cohort selection3, phenotyping4,o b s e r -
vational data analysis5, and predictive modeling6.
Despite the amount of valuable in formation in EHRs, extracting
information from clinical notes remains challenging7,8. Clinical information
extraction comprises several tasks, including named entity recognition
(NER) (e.g., recognizing “t2dm ”as type II diabetes mellitus)9,s e n s ed i s -
ambiguation (e.g., understanding “mi”as“myocardial infarction ”or“mitral
insufﬁciency ”depending on the context)10, and relation extraction (e.g.,
linking a symptom with medication if reported as a side-effect)11.
The simplest clinical information extraction approaches use rules and
dictionaries to identify entities of interest12,13, such as diagnosis codes like the
International Classi ﬁcation of Diseases (ICD). In a 2018 review of 263
clinical information extraction methods, 65% were rule-based8. These sys-
tems are interpretable, easy to deploy, and achieve reasonable performanceon many tasks
12. However, structured ﬁelds like diagnosis codes are unableto fully capture a patient ’s medical history in the current state. For example,
despite a recent increase in the use of diagnosis codes to represent social
determinants of health, they remain underutilized and often miss crucialcontextual details only found in the unstructured text of EHRs
14,15.M o r e -
over, for many conditions, such as cancer, ICD codes do not re ﬂect the true
source of diagnosis; in these cases, pathology reports are the goldstandard
16,17. Natural language processing methods are therefore necessary
to extract these insights, allowing fo r a more comprehensive understanding
of patient health. Additionally, hard-coded rules and word lists fail tocapture the wide variation in clinica l language, including synonyms and
abbreviations, and miss nuanced descriptions in EHR notes
18.
Supervised machine learning approaches that take in a labeled dataset
can recognize more complex linguistic relationships than rules- or
dictionary-based methods. Neural netw ork architectures like bi-directional
Long Short-Term Memory networks (LSTMs) are well suited for sequencedata-based tasks like NER, given their a bility to learn relationships between a
t o k e na n di t sn e i g h b o rt o k e n si ne i t h e rd i r e c t i o n
19. For example, Stanza20,21,a
widely used Python library for NER, uses a bi-directional LSTM with aConditional Random Field trained on the 2010 i2b2/VA dataset
22.A
1Stanford University School of Medicine, Stanford, CA, USA.2Department of Biomedical Data Science, Stanford, CA, USA.3Poolesville High School, Poolesville,
MD, USA.4Stanford Center for Biomedical Informatics Research, Stanford, CA, USA.5Division of Hospital Medicine, Stanford University School of Medicine,
Stanford, CA, USA.6Division of Clinical Informatics, Stanford University School of Medicine, Stanford, CA, USA.7Department of Psychiatry and Behavioral
Sciences, Stanford University School of Medicine, Stanford, CA, USA.8Division of Pulmonary, Allergy, and Critical Care Medicine, Stanford University School of
Medicine, Stanford, CA, USA.9Center for Innovation to Implementation, VA Palo Alto Healthcare System, Menlo Park, CA, USA.10Department of Health Policy,
Stanford University, Stanford, CA, USA.11Technology and Digital Solutions, Stanford Healthcare, Palo Alto, USA.12Clinical Excellence Research Center, Stanford
University School of Medicine, Stanford, CA, USA.13Department of Medicine, Stanford, CA, USA.14These authors contributed equally: Ivan Lopez, Akshay
Swaminathan. e-mail: ivlopez@stanford.edu
npj Digital Medicine |            (2025) 8:45 11234567890():,;
1234567890():,;
disadvantage of machine learning ap proaches is that they often require
large, labeled training datasets, which can be time-consuming and expensiveto obtain. Weak supervision offers an alternative to human-labeled data,
where programmatic labeling functio ns are used to automatically assign
“weak ”labels. Although the quality of weak labels is lower than human
l a b e l s ,t r a i n i n go nal a r g en u m b e ro fw e a kl a b e l sh a sb e e ns h o w nt oo u t -perform training on a small number of human labels. Labeling functions canbe manually curated or sourced from ontologies or smaller models. For
example, TROVE uses ontologies like the Uni ﬁed Medical Language System
(UMLS) to create labeling functions and uses these weak labels to ﬁne-tune a
BERT-based model for identifying symptoms and risk factors forCOVID-19
23.
Pre-trained deep learning models like BERT use multidimensional
embeddings learned from large, unlabeled corpuses. These embeddingsrepresent semantic information that can be used as features for a variety of
downstream tasks. For instance, ﬁne-tuned BERT-based models have been
employed for tasks including named entity recognition, assertion statusdetermination, sense disambigua tion, and relation extraction
24,25,a n da r e
often adapted to speci ﬁc clinical domains, like radiology26,27.A l t h o u g ht h e s e
models can be ﬁne-tuned to perform tasks like diagnostic code assignment,
treatment assignment28, and open-ended reasoning29–32, decoder-only
models are typically better equipped for this task.
Recently, large language models (LLMs) trained with transformer-
based architectures on large unlabeled text corpuses have demonstratedimpressive performance on both information extraction and natural lan-guage understanding tasks, such as information extraction (e.g., “does this
patient have diabetes? ”)
33, text summarization (e.g., “summarize this
patient ’sh i s t o r y ”)34, and conversational capabilities (e.g., “draft a response to
this patient ’s message ”)35. One advantage of LLMs is their “few-shot ”and
“zero-shot ”prompting capabilities, enabling them to accomplish tasks with
few to no labeled examples —tasks that previously required training or ﬁne-
tuning separate models with labeled datasets36–38. Recent work has used
LLMs to extract clinical variables fr om EHR notes, including social deter-
minants of health, medications, and postpartum hemorrhage4,39,40.W h i l e
LLMs show great promise in clinical information retrieval, they face severallimitations. For instance, the length of patient notes can surpass an LLM ’s
context window —the amount of text that can be passed into the model.
Naive approaches like truncation or selecting only documents that ﬁtw i t h i n
the context window risk excluding valuable information
4,39,41. Dividing a
note into smaller chunks with adjoini ng strides can address context window
limits but still requires multiple LL M queries per patient, which can be
computationally expensive. In addit ion, LLM performance has been shown
to degrade on reasoning tasks as input length increases, even on models with
large context windows42–45, suggesting that inputting long EHR excerpts
containing extraneous information can reduce performance.
Retrieval-augmented generation ( RAG) attempts to address this lim-
itation by retrieving and appending que ry-relevant information to the input
context. The retriever typically uses an encoder model to represent both the
query and reference information in em bedding space and retrieve infor-
mation whose embeddings are close to that of the query46.S o m eR A G
workﬂows embed small chunks of text that can ﬁtw i t h i nt h em o d e l ’sc o n t e x t
window and store those embeddings in a database for downstream retrieval.
Other approaches, mostly explored in the general domain, involveembedding and retrieving fact triplets from knowledge graphs
47–49.
An important challenge in RAG-based methods is ensuring that the
retrieved information is relevant to the query and does not contain extraneousinformation that can hinder LLM reasoning and add to inference costs
50,51.
To address the above limitations of LLMs for clinical information
extraction, we propose CLinical Enti ty Augmented Retrieval (CLEAR), a
RAG pipeline that retrieves note chunks containing clinical entities relevant
to the input query. We hypothesized that retrieval based on relevant clinical
entities would lead to more ef ﬁcient and relevant information retrieval
compared to RAG approaches based on note chunk embeddings. We makethree contributions. First, we validate the entity recognition and entityselection steps of the CLEAR pipeline, which identify clinical entities inclinical notes and select a subset relevant to the input query. Second, we
compare CLEAR to a RAG approach that embeds note chunks and a full-note retrieval approach in performing information extraction for 18 clinicalvariables. Third, we explore the feasibility of using CLEAR to generate labels
toﬁne-tune a BERT-sized model in performing information extraction. We
conduct all experiments on two real-world EHR-derived datasets thatinclude labels for substance use (e.g., alcohol dependence, tobacco depen-dence), mental health (e.g., attention-de ﬁcit/hyperactivity disorder
[ADHD], bipolar disorder, depression), social determinants of health (e.g.,
homelessness, unemployment), and chest radiograph ﬁndings (e.g., pneu-
monia, cardiomegaly).
Results
Inter-rater reliability
In the Stanford MOUD dataset, the unweighted Cohen ’sK a p p av a l u ew a s
0.86 (95% CI: 0.79-0.93). In the CheX pert dataset, the unweighted Cohen ’s
Kappa was 0.93 (95% CI: 0.88 –0.98). These values indicate excellent
agreement between annotators.
NER and entity selection evaluation
Zero-shot NER using Flan-T5 identi ﬁed 1269 out of 1382 entities (96%
sensitivity) in the NCBI disease dataset and 440 out of 450 entities (99%sensitivity) in the Stanford MOUD da taset. We measured to what extent
ontology and LLM augmentation recover entities missed in the NER step(false negatives) by using the UM LS ontology and GPT-4 to generate
synonyms as if they were the target entity. This augmentation step increases
sensitivity to 99% and 100% in the NCBI disease dataset and Stanford
MOUD dataset, respectively, indicating that even if the target entity ismissed during NER, it is very likely to be detected through ontology andLLM augmentation (Supplementary Table 1). The performance of each ofthe four zero-shot NER prompts on the Stanford MOUD dataset is detailedin Supplementary Table 2. We report the classi ﬁcation of false negatives for
this analysis in Supplementary Table 3.
We studied the impact of the initial NER step on the overall perfor-
mance of CLEAR. Overall, removing NER from CLEAR and relying only onontology and LLM augmentation hurts d ownstream information extraction
task performance, resulting in a 0.11 d ecrease in F1 across all 13 variables in
the Stanford MOUD dataset (0.86 without NER vs. 0.97 with NER). For
unhoused, personality disorder, ADH D, PTSD, suicidal behavior, liver
disease, and unemployment, removing NER resulted in an F1 drop of ≤0.02.
For other variables, removing NER l ed to a drop in F1 from 0.18 (bipolar
disorder) to 0.40 (substance use disorder) (Fig. 1and Supplementary Table
4). This suggests that for several variables, LLMs and ontologies do notcapture the natural variation in clinical variables as effectively as NER. For afull list of high-yield terms missed by Ontology +LLM augmentations, refer
to Supplementary Table 5.
Information extraction evaluation
On the Stanford MOUD Dataset, the average F1 score across all 13 variablesand all 6 models was 0.90, ranging from 0.78 (Med42) and 0.97 (GPT-4)across models. GPT-4 had the highest F1 score for 10 out of 13 variables.F1 scores across variables ranged from 0.61 (Med42 on depression) to 1.00(GPT-4 on personality disorder, bipol ar disorder, PTSD, and unemployment;
L l a m a - 3o nu n h o u s e d ;F l a n - T 5o nu n e m p l o y m e n t ) .O nt h eC h e X p e r tDataset, the average F1 score across a l l5t e s ts e t sf o ra l l6m o d e l sw a s0 . 9 6 ,
ranging from 0.91 (Flan-UL2) and 0.98 (Flan-T5 and Mixtral). Flan-T5 hadthe highest F1 score for 3 out of 5 variables. F1 scores across variables rangedfrom 0.90 (Med42 on pneumothorax) to 1.00 (Flan-T5 on cardiomegaly andpleural effusion) (Table 1). A full breakdown of CLEAR, chunk embedding,
and full-note performance per variable is reported in Supplementary Table 6.
We used CLEAR to label a dataset to ﬁne-tune a Bio +Clinical BERT
for information extraction of the 13 variables in the Stanford MOUD dataseta n d5v a r i a b l e si nt h eC h e X p e r td a t a set. Within the 13 Stanford MOUD
Dataset classi ﬁers, two showed perfect discrimination on the test set
(AUC = 1). The “suicidal behavior ”classi ﬁer had the lowest AUC (0.83).https://doi.org/10.1038/s41746-024-01377-1 Article
npj Digital Medicine |            (2025) 8:45 2
Within the CheXpert Dataset, the “cardiomegaly ”classi ﬁer had the highest
AUC (AUC = 1), and the “pulmonary edema ”classi ﬁer had the lowest AUC
(AUC = 0.97) (Supplementary Table 7). Using a predicted probabilitythreshold of 0.5, the ﬁne-tuned BERT model ’s F1 scores were consistently
within the range of the larger models ’F1 scores. For alcohol dependence and
chronic pain, the ﬁne-tuned BERT model F1 was higher than the trainer
model ’sF 1s c o r e( T a b l e 1).
Additionally, results from the wea k labeling experiments suggest that
our CLEAR outperforms weak supervisi on using regular expressions, which
resulted in lower average F1 scores compared to all LLMs used with CLEAR(Supplementary Table 8).
Comparison to chunk embedding and full-note approaches
Across all models, chunk embedding (top-5) and full-note methods per-
formed worse on the information ex traction task compared to CLEAR
(Supplementary Table 9). The performance delta was largest for GPT-4(average F1 0.97 CLEAR vs. 0.88 chunk embedding vs. 0.90 full note) and
smallest for Flan-T5 (average F1 0.91 CLEAR vs. 0.88 chunk embedding vs.0.88 full note) (Fig. 2a, Supplementary Table 9). Increasing top-k improves
chunk embedding performance, but even with k= 10, CLEAR out-
performed chunk embedding across all models except Med42, where thechunk embedding approach outperformed CLEAR by 0.01 (F1 0.79 vs. 0.78)(Supplementary Table 10). We conducted additional experiments byincreasing the CLEAR context window size from +/−150 words to +/−
185 words, and reducing token chun ks for chunk embeddings from 490 to
390. As a result, the token counts for C LEAR became larger than those for
chunk embedding. We re-ran our analysis using Mixtral, and the results
showed that the average F1 score for CLEAR increased by 0.01, while the
average F1 score for chunk embedding decreased by 0.02 (SupplementaryTable 11), compared to the original results in Supplementary Table 9.
CLEAR outperformed chunk embedding and full-note approaches on
nearly all ef ﬁciency metrics. Average inference time per note ranged fromTable 1 | CLEAR F1 scores for information extraction on the Stanford MOUD and CheXpert datasets
Variable Flan-T5 Flan-UL2 GPT-4 Med42 Llama-3 Mixtral Range Fine-tuned BERT
CheXpert
Cardiomegaly 1.00 0.96 0.95 0.97 0.95 0.99 1.00 –0.95 0.95
Pulmonary edema 0.98 0.96 0.96 0.91 0.96 0.98 0.98 –0.91 0.97
Pleural effusion 1.00 0.84 0.97 0.97 0.98 0.98 1.00 –0.84 0.89
Pneumonia 0.95 0.84 0.99 0.88 0.94 0.95 0.99 –0.84 0.94
Pneumothorax 0.98 0.95 0.99 0.90 0.97 0.98 0.99 –0.90 0.96
Average 0.98 0.91 0.97 0.93 0.96 0.98 0.98 –0.91 0.94
Stanford MOUD
Depression 0.86 0.87 0.97 0.61 0.88 0.93 0.97 –0.61 0.91
Alcohol dependence 0.85 0.81 0.91 0.69 0.74 0.75 0.91 –0.69 0.91a
Substance use disorder 0.89 0.88 0.91 0.71 0.84 0.94 0.94 –0.71 0.87
Unhoused 0.97 0.97 0.97 0.96 1.00 0.97 1.00 –0.96 0.94
Tobacco dependence 0.95 0.98 0.99 0.70 0.90 0.92 0.99 –0.70 0.90
Personality disorder 0.81 0.90 1.00 0.67 0.97 0.86 1.00 –0.67 0.95
Bipolar disorder 0.90 0.94 1.00 0.91 0.89 0.94 1.00 –0.89 0.90
PTSD 0.95 0.95 1.00 0.89 0.96 0.94 1.00 –0.89 0.85
ADHD 0.94 0.97 0.97 0.77 0.87 0.84 0.97 –0.77 0.77
Suicidal behavior 0.96 0.95 0.99 0.83 0.91 0.97 0.99 –0.83 0.87
Liver disease 0.82 0.97 0.99 0.62 0.81 0.94 0.99 –0.62 0.89
Chronic pain 0.95 0.97 0.95 0.88 0.94 0.94 0.97 –0.88 0.98a
Unemployment 1.00 0.98 1.00 0.88 0.84 0.95 1.00 –0.84 0.98
Average 0.91 0.93 0.97 0.78 0.89 0.91 0.97 –0.78 0.90
aFine-tuned BERT F1 score higher than the trainer model ’s F1 score on the same held-out test set.
Fig. 1 | CLEAR information retrieval ablation F1 scores on Stanford MOUD dataset. F1 scores for information retrieval using NER, Ontology, LLM augmentation, or
Ontology +LLM Augmentation on the Stanford MOUD Dataset. F1 scores were calculated for all 13 variables using GPT-4.https://doi.org/10.1038/s41746-024-01377-1 Article
npj Digital Medicine |            (2025) 8:45 3
1.04 s (Flan-T5) to 10.24 s (Med42) for CLEAR; from 4.92 s (Flan-T5) to
35.07 s (Med42) for chunk embedding; and from 7.20 s (Flan-UL2) to42.43 s (Med42) for full note. The ave rage number of model calls per note
was 1.68 for CLEAR vs. 4.94 for chunk embedding. These numbers were thesame across models since all models were called once per retrieved chunk.F o rt h ef u l l - n o t ea p p r o a c h ,t h en o t ew a sc h u n k e da c c o r d i n gt ot h ec o n t e x twindow of each model. As a result, mo dels with large input token limits —
like GPT-4 (125k) and Mixtral (128k) —required fewer model calls. The
average number of input tokens per note was substantially less in CLEAR
compared to chunk embedding and full note. On average, CLEAR had 81%fewer input tokens than the full-note approach and 71% fewer input tokens
than chunk embedding (Fig. 2b–d and Supplementary Table 12). We esti-
mated the time required for human evaluators to extract the same infor-mation from the training data by recording how long it took our domainexpert to annotate 100 notes. On average, it took 57 s for a human toannotate one variable in a clinical note, which would result in approximately3299 h to complete the annotation of 13 variables in 16,031 notes. Incomparison, CLEAR would process approximately 1.681 note chunks pernote, resulting in 26,948 model calls for this same annotation task. Our
fastest model takes an average time of 1.039 s per note chunk, reducing the
task to 101 h, while the slowest model tak es approximately 10.241 s, totaling
997 h. This represents a 96.9% ef ﬁciency gain with the fastest model and a
69.8% gain with the slowest, com pared to human annotation.
We calculated ROUGE-L F-measures to test whether chunk embed-
ding performed worse at information e x t r a c t i o nw h e nt h er e t r i e v e dt e x t
overlapped less with the text retrieved by CLEAR. When both CLEAR andchunk embedding succeeded (true positi ves and true negatives), the average
ROUGE-L was 77%. When CLEAR succeeded, but chunk embedding failed(false positives and false negatives), ROUGE-L was also 77%, suggesting thatperformance differences cannot be attributed to lack of overlap in theretrieved text ( p-value > 0.05). However, the average top-k ranks for TPs and
TNs with the highest F-measure (TPs = 3.12, TNs = 4.08) were more
favorable than those for FPs and FNs (FPs = 4.11, FNs = 5) ( p-value = 0.01),
indicating that the embedding similarity measure used by the chunkembedding method may not effectively prioritize the most relevant chunks(Supplementary Table 13). Overall, while both CLEAR and chunkembedding methods retrieve similarly high-yield content, CLEAR proves to
be a more ef ﬁcient information retrieval tool, returning relevant content in
fewer chunks (4.94 average chunk embedding chunks per note vs. 1.681average CLEAR embedding chunks per note) (Supplementary Table 12).
Discussion
In this paper, we propose CLEAR, a RAG pipeline that retrieves noteexcerpts containing clinical-named ent ities relevant to the input query. We
show that CLEAR, when used for extraction of 13 variables from clinical
notes, outperformed chunk embedding and full-note approaches, achieving
3% higher F1 on average with 71% fewer input tokens, 72% faster inferencetime, and 66% fewer model queries. We also demonstrated that CLEARoutputs can be used to ﬁne-tune BERT-sized models for variable extraction,
resulting in performance comparable to larger models.
Our analysis suggests that CLEAR outperforms chunk embedding and
full-note approaches for two main reasons. First, CLEAR retrieves shortercontext segments. Prior studies have shown that longer contexts candegrade LLM performance. For example, in the FlenQA dataset, whichinvolves three reasoning tasks, Levy et al. observed that as input length
increases, model performance deteriorates regardless of whether the key
information is located at the beginning, middle, or end of the input context,and that degradation occurs well befo re reaching the context limit of the
models
42. Similarly, Liu et al. report the “lost in the middle ”phenomenon,
where LLMs perform worse when key inf ormation is buried in the middle of
the input context compared to being at the beginning or end52.T h e ya l s o
noted that models with longer context capabilities, such as the 16k versionsof GPT-3.5, did not outperform shorter context models. In our own analysis,models like Mixtral, Llama, and GPT-4, despite having context windowslarge enough to accommodate multip le notes, did not perform as well as
CLEAR when processing the full note.
Second, we noted that the embedding model tends to rank chunks
differently than CLEAR, often downra nking critical chunks. This observa-
tion is consistent with our ﬁndings that chunk embedding performance
improves as the number of chunks retrieved increases from 3 to 5 to 10. Notethat we processed each chunk in separate model calls rather than within asingle large context. Prior research supports the idea that retrieval of most
Fig. 2 | LLM information extraction comparisons on Large Token Stanford
MOUD Dataset. Average F1 Score comparison between CLEAR and full-note or
chunk embedding approach. F1 scores were averaged across our 13 held-out test sets.P-values re ﬂect the Wilcoxon Signed-Rank Test on F1 scores across all 13 held-out
test sets between CLEAR and full-note or chunk embedding comparisons ( a). Chunk
embedding top-k equals 5 in these experiments. We evaluated average inference timeper note ( b), average model queries per note ( c), and average input tokens per note
(d) on the Large Token Stanford MOUD Dataset across full note, chunk embed-
dings, and CLEAR methods for ﬁve models. Chunk embedding top-k equals 5 in
these experiments. All metrics are calculated on 4xNVIDIA A100 80GB GPUs. Tocalculate the total tokens retrieved for GPT-4, we used Med42 as the representative
tokenizer. *p< 0.05, **p< 0.01.https://doi.org/10.1038/s41746-024-01377-1 Article
npj Digital Medicine |            (2025) 8:45 4
s i m i l a rd o c u m e n tc h u n k si sn o ta l w a y so p t i m a l .F o ri n s t a n c e ,G a ne ta l .
propose METRAG, which combines a similarity model with a utility modelfor retrieval, ﬁnding that their approach outper forms traditional similarity-
based RAG approaches across various QA datasets.
CLEAR ’s use of NER aligns with a robust precedent in RAG meth-
odologies. A recent review of RAG approaches included 16 studies thatincorporate entity recognition and entity-based reasoning in different waysfor RAG
53. For instance, NER can be employed to edit or revise generated
content. In CBRKBQA, NER aids in revising results by aligning generatedrelations with those in the local neig hborhood of the query entity within a
knowledge graph
54. Similarly, GMT-KBQA re-ranks retrieved entities and
relations and conducts relation classi ﬁcation and entity disambiguation
prior to generation55. Beyond content revision, several approaches use
entities to extract information dire ctly from knowledge graphs. For exam-
ple, FC-KBQA, StructGPT, and KAPING retrieve relevant triplets and facts
based on entity matching49,56,57. Xu et al. search across entities to identify
relevant subgraphs in knowledge graphs for customer support issues58,a n d
KnowledgeNavigator leverages NER for iterative ﬁltering of relations to
retrieve pertinent triplets from knowledge graphs59.F u r t h e r m o r e ,R H O
integrates entity embedding with knowledge graph embeddings to enhancedialog generation
60. These methodologies underscore the versatility of NER
in RAG, not only for retrieving information but also for structuring andreﬁning content generation. NER can al so be used to facilitate automated
knowledge graph generation, suggest i n gt h a tC L E A Rc o u l db eu s e dt ob o t h
generate knowledge graphs
61and retrieve from them to improve LLM
performance47–49.
Our study faces certain limitations. Fi rst, we restricted our evaluation to
the task of clinical variable extraction. Future research should explore theperformance of CLEAR on other tasks that can bene ﬁt from retrieval,
including summarization, question answ ering, and clinical reasoning. Uti-
lizing benchmarks like MedAlign
62can provide a more comprehensive
evaluation of CLEAR ’s capabilities across a broader range of tasks. Second,
in our chunk embedding comparison, we segmented chunks based on thecontext window of the embedding model. While this approach is consistentwith prior methods
3, it is possible that using different-sized embedding
chunks could yield similar accuracy to CLEAR. However, in our experiment,where we increased the CLEAR token size and decreased the chunk
embedding token size, CLEAR still outperformed chunk embedding (see
Supplementary Table 11). These ﬁndings are consistent with the data in
Supplementary Table 13, where CLEAR and chunk embedding methods donot retrieve substantially differen t information, and increasing CLEAR ’s
context size does not negatively impact CLEAR ’s performance. Instead,
chunk embedding underperforms bec ause the embedding similarity mea-
sures may not effectively prioritize the most relevant chunks. We believefurther exploration is warranted, alt hough a deep dive was beyond the scope
of this paper. Future experiments sho uld investigate the impact of chunk
size tuning on performance. Third, our task required the information
retrieval LLM process only one note chunk at a time. This can be adapted if
the task requires extracting information from multiple note types. Several
CLEAR note chunks from different notes can be combined into a singleprompt for LLM inference, however, this was not explored in our paper.Fourth, additional prompt tuning for CLEAR steps (NER, LLM augmen-tation, and entity selection) is needed for full optimization, and languagecould have been made more consisten t between the prompts used at dif-
ferent stages of the pipeline. Fifth, changes in data over time are inherent inmedical studies. The data split we select ed resulted in a higher proportion of
COVID and post-COVID era notes in th e Stanford MOUD testing dataset,
which may contain a higher proportion of notes re ﬂecting worsened mental
health among patients
63. Although we made efforts to check for imbalances
in our training and testing datasets, thes e inherent differences may still exist
and could impact our evaluation. Lastly, our analysis did not incorporate
model quantization methods for LLM inference. Implementing model
quantization could strike a balance between ef ﬁciency and performance,
making it a valuable area for future research. By optimizing model con ﬁg-
urations through quantization, we can enhance scalability and applicabilityin diverse contexts without compromising on performance, thereby pro-
viding more comprehensive insights into the optimal use of CLEAR inclinical information extraction.
Traditional methods of using LLM for clinical information extraction
are time-consuming and cost-prohib itive. Our work introduces a more
efﬁcient RAG pipeline that identi ﬁes relevant note chunks using clinical
NER before performing variable extr action, leading to a more than 70%
reduction in both token usage and pr ocessing time. Importantly, these
efﬁciencies were achieved with a sligh t gain in performance when compared
to approaches that utilize entire documents or embed note chunks forretrieval. This work demonstrates that the application of LLMs in healthcarecan be made more affordable and practical. We have validated this methodin the context of variable extraction, showing its potential to transform thelandscape of clinical information processing in healthcare settings.
Methods
Data source
We used data from two EHR-derived datasets from Stanford Hospital. The
ﬁrst was the Stanford Medication for Opioid Use Disorder (MOUD)
cohort6,64. This cohort includes data from patients treated for opioid use
disorder at Stanford Hospital between 2009 and 2023. Patients aged 18 –89
who were prescribed buprenorphine-naloxone for more than a day wereincluded. The cohort was split into a trai ning and testing dataset by treatment
start dates, using data up to 2020 for training and from 2021 onwards fortesting. The treatment start date was used to split the data to simulate data cut-offs that would be expected in a real-world deployment of CLEAR. To evaluate
the similarity between the training and testing data after the date split, we
analyzed the proportion of key concept mentions (unemployment, home-lessness, food insecurity, substance depe ndence, suicidal ideation, depression,
overdose) in both sets. Using a regular expression search with synonyms foreach variable (Supplementary Table 14), we found minor differences in
concept mentions across the datase ts (Supplementary Table 15). There were
767 patients in the training dataset with 16031 unique notes and 505 patientsin the testing dataset with 12319 unique notes. Combined, the testing andtraining datasets had a min, median, and max token lengths of 218, 1778, and10,981, respectively. Thirteen variab les were selected for manual annotation
by a board-certi ﬁed addiction medicine physician due to their importance in
delivering medication-ass isted therapy. These variables included clinical
diagnoses (depression, alcohol depend ence, substance use disorder, ADHD,
bipolar disorder, chronic pain, liver dis ease, personality disorder, PTSD, sui-
cidal behavior, tobacco dependence) and social determinants of health(housing and employment stat us. All data were de-identi ﬁed using the Safe
Harbor method according to NIST guidelines, with clinical text undergoingadditional anonymization via the TiDE algorithm
65. Approval for the study
was obtained from the Stanford Universi ty Institutional Review Board, pro-
tocol number 67423. This study was an an alysis of routinely collected EHR
data, and posed no additional risk to patients.
The second data source was CheXpert, a dataset of radiology reports
from Stanford Hospital66with programmatic labels for ﬁve well-de ﬁned
clinical entities commonly found in ch est x-ray reports: cardiomegaly,
pulmonary edema, pleural effusion, pneumonia, and pneumothorax. Theseﬁve were selected at random out of the 14 labeled observations in CheXpert.
We downsampled the CheXpert dataset i nto a testing and training dataset
by using the existing CheXpert agent ’s labels to randomly sample from the
larger CheXpert dataset. For the trai ning dataset, we randomly sampled 700
notes for each of the ﬁve selected clinical entities. We only sampled notes
that the CheXpert agent labeled as “present ”or“negated ”to upsample notes
with information relevant to our retrieval task. We used a similar approachfor the testing dataset, sampling 200 notes per entity, 100 of which had beenlabeled by CheXpert as “present ”and 100 as “negated ”. After selection,
CheXpert labels were discarded for both datasets. In total, we had 3500
patients containing 3500 unique notes in the testing dataset, and 1000patients containing 1000 unique notes i n the training dataset. Combined,
the testing and training datasets had a min, median, and max token length of41, 189, and 1025, respectively.https://doi.org/10.1038/s41746-024-01377-1 Article
npj Digital Medicine |            (2025) 8:45 5
To prevent data leakage, we removed testing notes for any patients
whose IDs were present in the traini ng data. This step ensured that no
patient appeared in both the testing a nd training datasets for the Stanford
MOUD and CheXpert tasks.
Data annotation
Five board-certi ﬁed physicians and one medical student collaboratively
performed manual annotation of clinical variables to obtain reference labels.
We randomly sampled 420 unique notes from the Stanford MOUD testingdataset to generate reference labels fo r 13 clinical entities outlined in Sup-
plementary Table 16. To reduce class i mbalance skewed towards negative
and absent cases, we ﬁltered the 420 unique note s using patient-level
structured data (ICD-10 codes) and a regular expression search, returningnotes containing any of the speci ﬁed strings or from patients with at least
one relevant ICD-10 code (Supplementary Table 17). Ultimately, we created13 individual annotati on datasets. The labels generated from these 13
datasets were used as our held-out test sets. 247 notes (20 from each dataset)were randomly selected for duplicate annotation to calculate inter-raterreliability (IRR). For the CheXpert test set, we generate referencelabels for all1000 notes outlined in Supplementary Table 1. 100 notes from this subset
were randomly selected for duplicate annotation to calculate inter-raterreliability. For the Stanford MOUD and CheXpert datasets, labelers received
speciﬁc instructions that outlined the crite ria for annotating each variable.
Their task involved identifying and lab eling notes for the presence, absence,
or uncertainty of a variable.
For each annotated variable, anno tators received instructions to
improve consistency. Instructions for the Stanford MOUD Dataset anno-
tation task can be found in Supplementary Table 18. Annotators labeledpositive mentions of a variable as present. Negation or absent mentions wereboth treated similarly. Ambiguous instances were marked as uncertain. 12notes in the Stanford MOUD Dataset and 8 notes in CheXpert had con-ﬂicting duplicate-annotated labels for IRR. These notes were excluded from
t h eh e l d - o u tt e s ts e t s .
To evaluate the sensitivity of our information retrieval pipeline, one
medical student annotated a special ized dataset known as the Stanford
MOUD NER Dataset. This dataset was created by randomly selecting 215zero-shot NER input texts from the St anford MOUD Dataset and manually
extracting clinically relevant enti ties and concepts. Instructions for the
annotation task can be found in Suppl ementary Table 19. We used this toevaluate the sensitivity of our informa tion retrieval pipeline on real-world
clinical datasets. After annotation, we had 450 unique clinical entities andconcepts for our evaluation. Details outlining the creation of the zero-shotNER input texts can be found below.
Full details on all datasets used in this study can be found in Supple-
mentary Fig. 1.
Clinical entity augmented retrieval
CLEAR uses NER to improve the accuracy and ef ﬁciency of clinical LLM
tasks. CLEAR takes in two inputs: clinical notes and entities of interest. Thepipeline begins with NER to identify all clinical entities within the notes.
Next, the identi ﬁed entities are ﬁltered down to those relevant to the entities
of interest. The ﬁltered list is then augmented using ontologies and LLMs to
increase sensitivity. The augmented list is fed to a target matcher thatretrieves a context window surrounding each relevant entity. The retrieved-context windows can be used for dow nstream tasks like summarization,
question answering, or information extraction. This multi-step approach isoutlined below and in Fig. 3.
Theﬁrst step in the CLEAR pipeline is identifying all clinical entities in
the input clinical notes using a NER mo del. The output of this step is a list of
unique clinical entities contained in the notes. We implemented the initialNER step using zero-shot NER with Flan-T5-XXL due to its specialized
NER instruction tuning
67. Most clinical NER models are domain-speci ﬁc
and are highly dependent on the dataset they were ﬁne-tuned on. To
minimize these limitations, we chose F lan-T5, a domain-agnostic model, for
NER. The model was run on a PHI-compliant virtual machine with8xNVIDIA L4 24GB GPUs. Given the 512 context window limit of Flan-T5,
we used prompts of fewer than 20 tokens and chunked input text to under100 tokens with a 15-token stride. We used four distinct prompts, and the
output-named entities from each prompt were aggregated and de-
duplicated to form the ﬁnal list of entities. Illustrations of each prompt
type are provided in Supplementary Fig. 2. Our approach leverages NERprompts to capture all clinically nam ed entities; however, users have the
ability to craft more focused NER prompts (e.g., “Return all named entities
related to congestive heart failure ”).
To evaluate the performance of our zero-shot NER approach, we used
two datasets: (1) the NCBI Disease Dataset, which contains annotations for1382 unique disease names and concepts
68, (2) the Stanford MOUD NER
Dataset, which contains annotations for 450 clinical entities and concepts.
Fig. 3 | Overview of CLEAR pipeline. CLEAR requires two inputs: (1) clinical notes
and (2) a target entity. Initially, our CLEAR implementation applies an NER modelto the clinical notes to extract a dataset of relevant entities. These entities are then
ﬁltered using word embeddings and cosine similarity to ensure relevance to thetarget entity. Next, additional entities related to the target entity are identi ﬁed using
ontologies and LLMs. The ﬁnal list of entities is used to retrieve note chunks through
regular expression matches. These chunks support a downstream LLM task (clinicalinformation extraction).https://doi.org/10.1038/s41746-024-01377-1 Article
npj Digital Medicine |            (2025) 8:45 6
We report the sensitivity of Flan-T5- XXL in identifying these entities.
Additionally, we characterize the false negatives into the followingcategories:
1.Acronym recognition failures : the model recognized either the full
term or an acronym for a concept, but not both (ex: “colorectal cancer ”
was identi ﬁed, but “crc”was missed).
2.Morphological variance failures : the model recognized either the
pleural or singular noun version of the concept, but not both(ex:“glioblastoma ”was identi ﬁed, but “glioblastomas ”was missed).
3.Partial failures : model failed to recognize the same concept in different
contexts (ex: “tay sachs disease ”,“tay sachs mutation ”,“ashkenazi tay
sachs disease ”,“tay sachs disease gene ”were identi ﬁed, but “tay sachs ”
was missed).
4.Other failures : zero-shot NER failures that do not fall into any of the
other categories (ex: “retinitis punctata albescens ”was missed).
We also investigated the impact of removing the NER step on the
overall performance of CLEAR. On all 13 variables in the Stanford MOUDDataset, we ran the CLEAR pipeline with and without the initial NER step.When running CLEAR without NER, the only entities selected are thoseidenti ﬁed during entity augmentation (described below) with ontologies
and LLMs. The selected entities were then used to retrieve sections of thenote that were passed to GPT-4 to extr act information about a variable of
interest. We report the average F1 of the information extraction task.
Once the unique clinical entities from all notes are identi ﬁed, the entities
relevant to the input target entity are s elected. These selected entities are
eventually used to retrieve relevant context windows for downstream LLM
tasks. First, all entities identi ﬁed via NER and the target entity were embedded
using Bio +Clinical BERT
69, and those entities with a cosine similarity ≥0.85
compared to the target entity were retai ned. We selected a cosine similarity
threshold of 0.85 after empirical testing attempting to balance the exclusion ofirrelevant entities with the retention of relevant ones. The resulting entities
were passed to GPT-4 with a prompt to ﬁlter the list to those most relevant to
the target entity (Supplementary Fig. 3). The entity ﬁltering step is modular,
allowing users to apply Bio +Clinical BERT cosine similarity, an LLM, a
human, or any combination to improve entity selection.
Next, the ﬁltered entity list is augmented to account for entities missed
during NER. Without an augmentatio n step, entities missed during NER
could lead to incorrect context retrieval downstream. For example, if the entity“lesch nyhan ”w a sm i s s e db yN E R ,a n dt h et a r g e te n t i t yi s “Lesch-Nyhan
syndrome ”, the downstream information retrieval might fail to retrieve sec-
tions of the note that mention “lesch nyhan ”. Here, we used the UMLS
ontology
70and GPT-4 to augment the list of entities from NER. We used the
search endpoint from the UMLS API to retrieve concept names related to thetarget entity, and retained concept n ames originating from the National
Library of Medicine Metathesaurus or SNOMED CT
71.W ea l s op r o m p t e d
GPT-4 to generate synonyms for the target entity (Supplementary Fig. 4).
To evaluate the impact of the entity augmentation step, we measured to
what extent ontology and LLM augmenta tion recover entities missed in the
NER step (false negatives). To evaluate the impact of the entity augmentation
step, we measured how well ontology an d LLM augmentation recover entities
missed in the NER step (false negatives), as minimizing false negatives is crucialfor downstream information retrieval. We prioritized maximizing sensitivity/
recall, as false positives can be managed by the downstream LLM task, whereasfalse negatives result in complete loss of information. For each entity missed by
the NER step, we treat a variant of the missed entity as the target entity and usethe UMLS ontology and GPT-4 to generate synonyms as described above. Forexample, the formal name of an entity was used ( “Lesch-Nyhan syndrome ”)i fa
variant was missed ( “lesch nyhan ”) .I ft h ef o r m a ln a m ew a st h em i s s e dt e r m ,a
broader term that would encompass the formal name ( “purine salvage de ﬁ-
ciencies ”) was used as the target entity. We report the proportion of entities
missed during NER that were recovere d through this augmentation step.
The selected entities are used to develop a regular expression tool for
information retrieval. Speci ﬁcally, we employed the Target Matcher pro-
vided by MedSpaCy
72. We used the TargetRule class from the MedSpaCyNER module for identifying mentions of the selected entities within clinical
notes and then pulled a context wind ow of 150 words before and after the
target entity. These retrieved-context windows are passed to an LLM fordownstream inference tasks.
Information extraction
We used the retrieved-context windows from CLEAR to extract the infor-mation (ex: is the feature present, negated/absent, or uncertain) of 13variables in the Stanford MOUD dataset and 5 variables in the CheXpertdataset. We compared the performance of several models with differentcontext windows on this task. These models included: Med42 –70b
73,
Mixtral-8x7B-Instruct-v0.174,L l a m a - 3 –70b75,F l a n - T 5 - X X L67,F l a n - U L 276,
and GPT-477. We ran GPT-4 via a secure Azure PHI-compliant instance.
The other ﬁve models were run on a PHI-compliant virtual machine with
4xNVIDIA A100 80GB GPUs.
We designed prompts that include d synthetic in-context examples
generated by GPT-478.W ei n c l u d e d ﬁve examples for each entity, covering all
possible labels that the LLM was to discern: “0”for entity negated/absence, “1”
for presence, and “2”for uncertainty. We selected 5-shot prompting based on
i t sd e m o n s t r a t e dp e r f o r m a n c eg a i n si np r i o rw o r k36. To mitigate any potential
issues with the context window limita tions of each model, we kept the syn-
thetic data points under 100 tokens. We kept each example under 100 tokensto ensure they provide meaningful insights into the task, improving LLMinstruction following without consumi ng excessive tokens. In comparing our
methods to the traditional approach, also known as the full-note method, we
accounted for the increase in token le ngth due to our prompting strategies,
ensuring the models ’token limits were not exceeded. An example of our LLM
i n f o r m a t i o ne x t r a c t i o np r o m p ti sp r o v i d e di nS u p p l e m e n t a r yF i g .5 .
For each model, we report classi ﬁcation metrics (sensitivity, speci ﬁcity,
NPV, PPV, F1) comparing the LLM information extraction labels tohuman-annotated labels.
Weak labeling
We compared CLEAR to a weak labeling approach. Weak supervision was
used to label the 2000 CheXpert notes in the training set for our ﬁve
CheXpert entities of interest. Labeling functions are rough heuristics used toprogrammatically generate weak label s from unlabeled data. We manually
reviewed the 250 notes in the training set and created labeling functions
using keyword matching and regular expressions from a list of synonymscreated by a domain expert and supple mented by GPT-4 using the prompt
in Supplementary Fig. 4. The ﬁnal list of synonyms can be found in Sup-
plementary Table 20. For example, a la beling function might label a note as
having cardiomegaly if it contains the strings “enlarged cardiac silhouette, ”
“enlarged heart, ”or“ventricular hypertrophy, ”and abstain otherwise. The
labeling functions for each entity we re used to train a model that combines
the outputs of multiple labeling functi ons for a given entity, leveraging their
collective knowledge and handling their con ﬂicts
79.
Model distillation
We investigated whether the output of CLEAR could be used to ﬁne-tune a
smaller language model to perform the information extraction task. Weused the output of CLEAR to ﬁne-tune BERT models to perform a binary
classi ﬁcation task (present vs. negated/abs ent) for each of the variables in the
Stanford MOUD and CheXpert datasets. We omitted the “uncertain ”class
due to small sample sizes in certain ﬁne-tuning datasets (Supplementary
Table 21). We ﬁne-tuned Bio +Clinical BERT, which was initialized from
BioBERT and trained on all MIMIC notes
69. For each variable, we selected
the best performing LLM, excluding GPT-4 on the information extractiontask to weakly label the ﬁne-tuning dataset (Supplementary Table 21). We
excluded GPT-4 since OpenAI terms of u se prohibit using GPT-4 outputs to
develop competitor models
80.T h e ﬁne-tuning dataset for each variable
consisted of every note chunk containing an entity of interest (inputs) andlabel generated by an LLM (label). We r emoved note chunks that contained
>10% overlapping words, resulting in less than 2% of note chunks beingﬁltered out.https://doi.org/10.1038/s41746-024-01377-1 Article
npj Digital Medicine |            (2025) 8:45 7
Theﬁne-tuning datasets for each variable were divided into a 70%
training set and a 30% validation set. Hyperparameters were tuned using 10-
fold cross-validation on 70% of the training data to maximize the area underthe receiver operating curve (AUC). We s elected a range of variables for each
hyperparameter and performed a grid search to ﬁnd the best hyperparameter
conﬁgurations. Our grid search included learning rate (5e-5, 3e-5, 2e-5),
batch size (8, 16, 32), number of training epochs (4, 5, 10), and weight decay(0.01, 0.05, 0.1). The ﬁnal models were ﬁne-tuned on 100% of the ﬁne-tuning
dataset. Performance metrics for the ﬁne-tuned classi ﬁer were generated
using the held-out test sets. To prevent data leakage, we removed testing notes
for any patients whose IDs were present in the ﬁne-tuning data (training and
validation datasets). This ensured that no patient appeared in both the held-o u tt e s ts e ta n d ﬁne-tuning datasets for the Stanford and CheXpert variables.
Comparison to chunk embedding and full-note approaches
To quantify the impact of retrieving text around entities, we compared
CLEAR to a RAG pipeline leveraging note chunk embeddings and a naiveapproach that retrieves the full note. We ﬁltered the test sets down to longer
notes to focus this comparison on no tes that approached or exceeded
models ’input context window. Speci ﬁcally, we select the 50% longest notes
in the Stanford MOUD Dataset.
For the chunk embedding RAG pipeline, we used the BAAI General-
ized Embeddings (BGE) model as our embedding model and cosine simi-larity as the retriever (Supplementary Fig. 6). BGE is a high-performanceembedding model known for its accuracy on retrieval benchmarks
81.W eﬁrst
segmented all patient notes into chunks of 490 tokens with a stride of 128
tokens, given BGE ’s maximum context window of 512. We then generated
embeddings for each chunk as well as every target entity and its de ﬁnition
and stored them in an embedding database. To select the most relevant notechunks for our information extraction t ask, we perform cosine similarity to
measure the alignment of each not e chunk against the target entity ’sd eﬁ-
nition embedding. We retrieved the top- k(where k=3 ,5 ,1 0 )n o t ec h u n k s
based on cosine similarity scores. For notes with fewer than k chunks, weretrieved all chunks. The retrieved chunks were passed to an LLM for theinformation extraction t ask. For notes with multiple chunks, we aggregated
the LLM labels from these chunks to generate a ﬁnal label for the note.
For the full-note approach, we chunked notes based on each model ’s
context window limit, using a stride of 128 tokens
4(Supplementary Fig. 7).
We passed each chunk to an LLM for the information extraction task andaggregated the labels from these segments to produce a ﬁnal label for
each note.
For all LLMs, we compare the performance of CLEAR, chunk
embeddings, and the full-note approach on the information extraction tasks
as well as on three metrics related to inference ef ﬁciency. For the infor-
mation extraction tasks, we report th e average inference time per note,
average model queries per note, and average tokens retrieved per note. Wedo not report inference time for GPT-4 since it was run using a proprietary
API. We used the Wilcoxon Signed-Rank Test to compare the differences
between the three methods
82.
We tested two hypotheses regarding performance differences between
CLEAR and chunk embedding. First, w e hypothesized that chunk embed-
dings would perform worse than CLEAR when the retrieved chunksoverlapped less with the chunks retrieved by CLEAR. To test this, we cal-culated ROUGE-L F-measure —a measure of the longest common sub-
sequence between two strings —on the chunks retrieved by CLEAR and
chunk embedding, treating the CLEA R chunks as the reference. We report
ROUGE-L for cases where both CLEAR and chunk embeddings succeeded(true negatives or true positives), and for cases where CLEAR succeeded butchunk embeddings failed (false negat ives and false positives). Second, we
hypothesized that chunk embeddings would perform worse than CLEAR
when the parts of the note retrieved by CLEAR were ranked lower by the
chunk embedding model. To do this, we calculated the chunk embeddingmodel ranking of the chunk that over lapped most with the CLEAR chunk
(as measured by ROUGE-L).Model summary
For NER, we relied on Flan-T5-XXL. LLM Augmentation used GPT-4. Ourentity selection cosine similarity model was Bio +ClinicalBERT, and the
entity ﬁltering LLM was GPT-4. Information extraction was tested on six
models: Med42 –70b, Mixtral-8x7B-Instruct-v0.1, Llama-3 –70b, Flan-T5-
XXL, Flan-UL2, and GPT-4. The chunk embedding model was BAAIGeneralized Embeddings Large English v1.5, and for model distillation, weﬁne-tuned Bio +ClinicalBERT (Supplementary Table 22). Model usage
parameters for NER and information extraction are reported in Supple-mentary Table 23.
Data availability
The Stanford MOUD Cohort Dataset used in this study contains identi ﬁable
protected health information and, the refore, cannot be shared publicly.
Stanford University investigators with appropriate IRB approval can contact
the authors directly regarding data access.
Code availability
The code used to run CLEAR and reproduce results can be found at https://
github.com/iv-lop/clear .
Received: 1 July 2024; Accepted: 8 December 2024;
References
1. Ross, M. K., Wei, W. & Ohno-Machado, L. Big data and the electronic
health record. Yearb. Med. Inform. 9,9 7–104 (2014).
2. Meystre, S. M., Savova, G. K., Kipper-Schuler, K. C. & Hurdle, J. F.
Extracting information from textual documents in the electronic healthrecord: a review of recent research. Yearb. Med. Inform. 17, 128–144
(2008).
3. Wornow, M. et al. Zero-shot clinical trial patient matching with LLMs.
Preprint at https://doi.org/10.48550/arXiv.2402.05125 (2024).
4. Alsentzer, E. et al. Zero-shot interpretable phenotyping of postpartum
hemorrhage using large language models. Npj Digit. Med. 6,1–10
(2023).
5. Callahan, A., Shah, N. H. & Chen, J. H. Research and reporting
considerations for observational studies using electronic health
record data. Ann. Intern. Med. 172, S79 –S84 (2020).
6. Lopez, I. et al. Predicting premature discontinuation of medication for
opioid use disorder from electronic medical records. Amia. Annu.
Symp. Proc. 2023 , 1067 –1076 (2024).
7. Zweigenbaum, P., Demner-Fushman, D., Yu, H. & Cohen, K. B.
Frontiers of biomedical text mining: current progress. Brief. Bioinform.
8, 358–375 (2007).
8. Wang, Y. et al. Clinical information extraction applications: a literature
review. J. Biomed. Inform. 77,3 4–49 (2018).
9. Lample, G., Ballesteros, M., Subramanian, S., Kawakami, K. & Dyer,
C. Neural Architectures for Named Entity Recognition. in Proceedings
of the 2016 Conference of the North American Chapter of theAssociation for Computational Linguistics: Human LanguageTechnologies (eds. Knight, K., Nenkova, A. & Rambow, O.) 260 –270
https://doi.org/10.18653/v1/N16-1030 (Association for
Computational Linguistics, San Diego, California, 2016).
10. Kågebäck, M. & Salomonsson, H. Word Sense Disambiguation using
a Bidirectional LSTM. in Proceedings of the 5th Workshop on
Cognitive Aspects of the Lexicon (CogALex - V) (eds. Zock, M., Lenci,
A. & Evert, S.) 51 –56 (The COLING 2016 Organizing Committee,
Osaka, Japan, 2016).
11. Wu, H. et al. A survey on clinical natural language processing in the
United Kingdom from 2007 to 2022. Npj Digit. Med. 5,1–15 (2022).
12. Jung, K. et al. Functional evaluation of out-of-the-box text-mining
tools for data-mining tasks. J. Am. Med. Inform. Assoc. JAMIA 22,
121–131 (2015).https://doi.org/10.1038/s41746-024-01377-1 Article
npj Digital Medicine |            (2025) 8:45 8
13. Percha, B. Modern clinical text mining: a guide and review. Annu. Rev.
Biomed. Data Sci. 4, 165–187 (2021).
14. Agarwal, A. R., Prichett, L., Jain, A. & Srikumaran, U. Assessment of
use of ICD-9 and ICD-10 codes for social determinants of health in the
US, 2011-2021. JAMA Netw. Open 6, e2312538 (2023).
15. Truong, H. P. et al. Utilization of social determinants of health ICD-10
Z-codes among hospitalized patients in the United States,2016–2017. Med. Care 58, 1037 (2020).
16. Swaminathan, A. et al. Selective prediction for extracting unstructured
clinical data. J. Am. Med. Inform. Assoc. 31, 188–197 (2024).
17. Liu, C. et al. Predictive value of clinical complete response after
chemoradiation for rectal cancer. J. Am. Coll. Surg. 235, S51 (2022).
18. Liao, K. P. et al. Development of phenotype algorithms using
electronic medical records and incorporating natural languageprocessing. BMJ 350, h1885 (2015).
19. Huang, Z., Xu, W. & Yu, K. Bidirectional LSTM-CRF models for
sequence tagging. Preprint at http://arxiv.org/abs/1508.01991
(2015).
20. Zhang, Y., Zhang, Y., Qi, P., Manning, C. D. & Langlotz, C. P.
Biomedical and clinical English model packages for the StanzaPython NLP library. J. Am. Med. Inform. Assoc. JAMIA 28, 1892 –1899
(2021).
21. Qi, P., Zhang, Y., Zhang, Y., Bolton, J. & Manning, C. D. Stanza: a
Python natural language processing toolkit for many humanlanguages. in Proceedings of the 58th Annual Meeting of the
Association for Computational Linguistics: System Demonstrations
(eds. Celikyilmaz, A. & Wen, T.-H.) 101 –108 (Association for
Computational Linguistics, Online). https://doi.org/10.18653/v1/
2020.acl-demos.14 (2020).
22. Uzuner, Ö., South, B. R., Shen, S. & DuVall, S. L. 2010 i2b2/VA
challenge on concepts, assertions, and relations in clinical text. J. Am.
Med. Inform. Assoc. JAMIA 18, 552–556 (2011).
23. Fries, J. A. et al. Ontology-driven weak supervision for clinical entity
classi ﬁcation in electronic health records. Nat. Commun. 12, 2017
(2021).
24. Jaber, A. & Martínez, P. Disambiguating clinical abbreviations using a
one- ﬁts-all classi ﬁer based on deep learning techniques. Methods Inf.
Med. 61, e28–e34 (2022).
25. Gu, Y. et al. Domain-speci ﬁc language model pretraining for
biomedical natural language processing. ACM Trans Comput.
Healthc .3,1–23 (2022).
26. Zambrano Chaves, J. et al. RaLEs: a Benchmark for Radiology
Language Evaluations. in Advances in Neural Information Processing
Systems (eds. Oh, A. et al.) vol. 36 74429 –74454 (Curran Associates,
Inc., 2023).
27. Yan, A. et al. RadBERT: adapting transformer-based language
models to radiology. Radiol. Artif. Intell. 4, e210258 (2022).
28. Sushil, M., Ludwig, D., Butte, A. J. & Rudrapatna, V. A. Developing a
general-purpose clinical language inference model from a large
corpus of clinical notes. arXiv.org https://arxiv.org/abs/2210.06566v1
(2022).
29. Lin, B. Y. et al. Differentiable Open-Ended Commonsense Reasoning.
inProceedings of the 2021 Conference of the North American Chapter
of the Association for Computational Linguistics: Human LanguageTechnologies (eds. Toutanova, K. et al.) 4611 –4625. https://doi.org/
10.18653/v1/2021.naacl-main.366 (Association for Computational
Linguistics, Online, 2021).
30. Klein, T. & Nabi, M. Attention is (not) all you need for commonsense
reasoning. https://arxiv.org/abs/1905.13497v1 (2019).
31. Li, L., Xin, X. & Guo, P. The exploration of the reasoning capability of
BERT in relation extraction. in 2020 10th International Conference on
Information Science and Technology (ICIST) 219–228. https://doi.org/
10.1109/ICIST49303.2020.9202183 (2020).
32. Amirizaniani, M., Martin, E., Sivachenko, M., Mashhadi, A. & Shah, C.
Do LLMs exhibit human-like reasoning? Evaluating theory of mind inLLMs for open-ended responses. https://arxiv.org/abs/2406.
05659v1 (2024).
33. Sushil, M. et al. CORAL: Expert-curated oncology reports to advance
language model inference. NEJM AI 1, AIdbp2300110 (2024).
34. Van Veen, D. et al. Adapted large language models can outperform
medical experts in clinical text summarization. Nat. Med .1–9https://
doi.org/10.1038/s41591-024-02855-5 (2024).
35. Tu, T. et al. Towards conversational diagnostic AI. Preprint at https://
doi.org/10.48550/arXiv.2401.05654 (2024).
36. Brown, T. et al. Language Models are Few-Shot Learners. in
Advances in Neural Information Processing Systems vol. 331877–1901 (Curran Associates, Inc., 2020).
37. Moor, M. et al. Foundation models for generalist medical arti ﬁcial
intelligence. Nature 616, 259–265 (2023).
38. Agrawal, M., Hegselmann, S., Lang, H., Kim, Y. & Sontag, D. Large
language models are few-shot clinical information extractors. in
Proceedings of the 2022 Conference on Empirical Methods in NaturalLanguage Processing (eds. Goldberg, Y., Kozareva, Z. & Zhang, Y.)
1998–2022. https://doi.org/10.18653/v1/2022.emnlp-main.130
(Association for Computational Linguistics, Abu Dhabi, United ArabEmirates, 2022).
39. Guevara, M. et al. Large language models to identify social
determinants of health in electronic health records. Npj Digit. Med. 7,
1–14 (2024).
40. Goel, A. et al. LLMs Accelerate Annotation for Medical Information
Extraction. in Proceedings of the 3rd Machine Learning for Health
Symposium 82–100 (PMLR, 2023).
41. Mahbub, M. et al. Leveraging large language models to extract
information on substance use disorder severity from clinical notes: azero-shot learning approach. Preprint at http://arxiv.org/abs/2403.
12297 (2024).
42. Levy, M., Jacoby, A. & Goldberg, Y. Same Task, More Tokens: the
Impact of Input Length on the Reasoning Performance of LargeLanguage Models. in Proceedings of the 62nd Annual Meeting of the
Association for Computational Linguistics (Volume 1: Long Papers)
(eds. Ku, L.-W., Martins, A. & Srikumar, V.) 15339 –15353. https://doi.
org/10.18653/v1/2024.acl-long.818 (Association for Computational
Linguistics, Bangkok, Thailand, 2024).
43. Shaham, U., Ivgi, M., Efrat, A., Berant, J. & Levy, O. ZeroSCROLLS: a
zero-shot benchmark for long text understanding. https://arxiv.org/
abs/2305.14196v3 (2023).
44. Bai, Y. et al. LongBench: A bilingual, multitask benchmark for long
context understanding. https://arxiv.org/abs/2308.14508v1 (2023).
45. Li, J., Wang, M., Zheng, Z. & Zhang, M. LooGLE: Can long-context
language models understand long contexts? https://arxiv.org/abs/
2311.04939v1 (2023).
46. Gao, Y. et al. Retrieval-augmented generation for large language
models: a survey. Preprint at https://doi.org/10.48550/arXiv.2312.
10997 (2024).
47. Agrawal, G., Kumarage, T., Alghamdi, Z. & Liu, H. Can Knowledge
Graphs Reduce Hallucinations in LLMs? : A Survey. in Proceedings of
the 2024 Conference of the North American Chapter of theAssociation for Computational Linguistics: Human LanguageTechnologies (Volume 1: Long Papers) (eds. Duh, K., Gomez, H. &
Bethard, S.) 3947 –3960. https://doi.org/10.18653/v1/2024.naacl-
long.219 . (Association for Computational Linguistics, Mexico City,
Mexico, 2024).
48. Wu, Y. et al. Retrieve-rewrite-answer: a KG-to-text enhanced LLMs
framework for knowledge graph question answering. Preprint athttp://arxiv.org/abs/2309.11206 (2023).
49. Baek, J., Aji, A. F. & Saffari, A. Knowledge-Augmented Language
Model Prompting for Zero-Shot Knowledge Graph QuestionAnswering. in Proceedings of the 1st Workshop on Natural Language
Reasoning and Structured Explanations (NLRSE) (eds. Dalvi Mishra,
B., Durrett, G., Jansen, P., Neves Ribeiro, D. & Wei, J.) 78 –106. https://https://doi.org/10.1038/s41746-024-01377-1 Article
npj Digital Medicine |            (2025) 8:45 9
doi.org/10.18653/v1/2023.nlrse-1.7 . (Association for Computational
Linguistics, Toronto, Canada, 2023).
50. Li, Y., Dong, B., Guerin, F. & Lin, C. Compressing Context to Enhance
Inference Ef ﬁciency of Large Language Models. in Proceedings of the
2023 Conference on Empirical Methods in Natural Language
Processing (eds. Bouamor, H., Pino, J. & Bali, K.) 6342 –6353. https://
doi.org/10.18653/v1/2023.emnlp-main.391 (Association for
Computational Linguistics, Singapore, 2023).
51. Mialon, G. et al. Augmented language models: a survey. Preprint at
http://arxiv.org/abs/2302.07842 (2023).
52. Liu, N. F. et al. Lost in the Middle: How Language Models Use Long
Contexts. Trans. Assoc. Comput. Linguist. 12, 157–173 (2024).
53. Zhao, P. et al. Retrieval-augmented generation for AI-generated
content: a survey. Preprint at http://arxiv.org/abs/2402.19473 (2024).
54. Das, R. et al. Case-based Reasoning for Natural Language Queries
over Knowledge Bases. in Proceedings of the 2021 Conference on
Empirical Methods in Natural Language Processing (eds. Moens, M.-
F., Huang, X., Specia, L. & Yih, S. W.) 9594 –9611. https://doi.org/10.
18653/v1/2021.emnlp-main.755 . (Association for Computational
Linguistics, Online and Punta Cana, Dominican Republic, 2021).
55. Hu, X., Wu, X., Shu, Y. & Qu, Y. Logical Form Generation via Multi-task
Learning for Complex Question Answering over Knowledge Bases. inProceedings of the 29th International Conference on ComputationalLinguistics (eds. Calzolari, N. et al.) 1687 –1696 (International
Committee on Computational Linguistics, Gyeongju, Republic ofKorea, 2022).
56. Zhang, L. et al. FC-KBQA: A ﬁne-to-coarse composition framework
for knowledge base question answering. in Proceedings of the 61st
Annual Meeting of the Association for Computational Linguistics(Volume 1: Long Papers) (eds. Rogers, A., Boyd-Graber, J. & Okazaki,
N.) 1002 –1017 (Association for Computational Linguistics, Toronto,
Canada, 2023). https://doi.org/10.18653/v1/2023.acl-long.57 (2023).
57. Jiang, J. et al. StructGPT: A General Framework for Large Language
Model to Reason over Structured Data. in Proceedings of the 2023
Conference on Empirical Methods in Natural Language Processing(eds. Bouamor, H., Pino, J. & Bali, K.) 9237 –9251 (Association for
Computational Linguistics, Singapore, 2023). https://doi.org/10.
18653/v1/2023.emnlp-main.574 (2023).
58. Xu, Z. et al. Retrieval-Augmented Generation with Knowledge Graphs
for Customer Service Question Answering. in Proceedings of the 47th
International ACM SIGIR Conference on Research and Developmentin Information Retrieval 2905 –2909 .https://doi.org/10.1145/
3626772.3661370 . (Association for Computing Machinery, New York,
NY, USA, 2024).
59. Guo, T. et al. KnowledgeNavigator: leveraging large language models
for enhanced reasoning over knowledge graph. Complex Intell. Syst.
10, 7063 –7076 (2024).
60. Ji, Z. et al. RHO: reducing hallucination in open-domain dialogues with
knowledge grounding. in Findings of the Association for
Computational Linguistics: ACL 2023 (eds. Rogers, A., Boyd-Graber,
J. & Okazaki, N.) 4504 –4522 (Association for Computational
Linguistics, Toronto, Canada, 2023). https://doi.org/10.18653/v1/
2023.
ﬁndings-acl.275 (2023).
61. Al-Moslmi, T., Gallofré Ocaña, M., Opdahl, A. L. & Veres, C. Named
entity extraction for knowledge graphs: a literature overview. IEEE
Access 8, 32862 –32881 (2020).
62. Fleming, S. L. et al. MedAlign: A Clinician-Generated Dataset for
Instruction Following with Electronic Medical Records. Proc. AAAI
Conf. Artif. Intell. 38, 22021 –22030 (2024).
63. Galea, S., Merchant, R. M. & Lurie, N. The mental health consequences of
COVID-19 and physical distancing: the need for prevention and early
intervention. JAMA Intern. Med. 180,8 1 7–818 (2020).
64. Nateghi Haredasht, F. et al. Predictability of buprenorphine-naloxone
treatment retention: A multi-site analysis combining electronic healthrecords and machine learning. Addiction 119, 1792 –1802 (2024).65. Datta, S. et al. A new paradigm for accelerating clinical data science at
Stanford Medicine. Preprint at https://doi.org/10.48550/arXiv.2003.
10534 (2020).
66. Irvin, J. et al. CheXpert: A Large Chest Radiograph Dataset with
Uncertainty Labels and Expert Comparison. Proc. AAAI Conf. Artif.
Intell. 33, 590–597 (2019).
67. Chung, H. W. et al. Scaling Instruction-Finetuned Language Models.
J. Mach. Learn. Res. 25,1–53 (2024).
68. Do ğan, R. I., Leaman, R. & Lu, Z. NCBI disease corpus: a resource for
disease name recognition and concept normalization. J. Biomed.
Inform. 47,1–10 (2014).
69. Alsentzer, E. et al. Publicly Available Clinical BERT Embeddings. in
Proceedings of the 2nd Clinical Natural Language ProcessingWorkshop (eds. Rumshisky, A., Roberts, K., Bethard, S. & Naumann,
T.) 72 –78.https://doi.org/10.18653/v1/W19-1909 . (Association for
Computational Linguistics, Minneapolis, Minnesota, USA, 2019).
70. Bodenreider, O. The Uni ﬁed Medical Language System (UMLS):
integrating biomedical terminology. Nucleic Acids Res 32,
D267 –D270 (2004).
71. Stearns, M. Q., Price, C., Spackman, K. A. & Wang, A. Y. SNOMED
clinical terms: overview of the development process and projectstatus. Proc. AMIA Symp .2001 , 662–666 (2001).
72. Eyre, H. et al. Launching into clinical space with medspaCy: a new
clinical text processing toolkit in Python. AMIA. Annu. Symp. Proc.
2021 , 438–447 (2022).
73. Christophe, C. et al. Med42 -- Evaluating Fine-Tuning Strategies for
Medical LLMs: Full-Parameter vs. Parameter-Ef ﬁcient Approaches.
Preprint at https://doi.org/10.48550/arXiv.2404.14779 (2024).
74. AI, M. Mixtral of Experts .https://mistral.ai/news /mixtral-of-experts/ (2023).
75. Gratta ﬁori, A. et al. The Llama 3 Herd of Models. Preprint at https://doi.
org/10.48550/arXiv.2407.21783 (2024).
76. Tay, Y. et al. Unifying Language Learning Paradigms .https://arxiv.org/
abs/2205.05131 (2023).
77. OpenAI et al. GPT-4 Technical Report .https://arxiv.org/abs/2303.
08774v6 (2023).
78. Dong, Q. et al. A survey on in-context learning. https://arxiv.org/abs/
2301.00234v3 (2022).
79. Ratner, A. et al. Snorkel: rapid training data creation with weak
supervision. Proc. VLDB Endow. 11, 269–282 (2017).
80. OpenAI is an AI research and deployment company. Terms of Use .
https://openai.com/policies/terms-of-use/ (2023).
81. Xiao, S., Liu, Z., Zhang, P. & Muennighoff, N. C-Pack: packaged
resources to advance general Chinese embedding. Preprint at https://
doi.org/10.48550/arXiv.2309.07597 (2023).
82. Wilcoxon, F. Individual comparisons by ranking methods. Biom. Bull.
1,8 0–83 (1945).
Acknowledgements
Dr. Chen has received research funding support in part by the NIH/National
Institute of Allergy and Infectious Diseases (1R01AI17812101), NIH/NationalInstitute on Drug Abuse Clinical Trials Network (UG1DA015815-CTN-0136),Gordon and Betty Moore Foundation (Grant #12409), Stanford Arti ﬁcial
Intelligence in Medicine and Imaging —Human-Centered Arti ﬁcial
Intelligence (AIMI-HAI) Partnership Grant, American Heart Association —
Strategically Focused Research Network —Diversity in Clinical Trials, NIH-
NCATS-CTSA grant (UL1TR003142) for common research resources. Thecontent is solely the responsibility of the authors and does not necessarilyrepresent the of ﬁcial views of the NIH, Stanford Healthcare, or any other
organization.
Author contributions
Conceptualization: I.L., A.S., K.V., F.N.H. Supervision: J.H.C., N.H.S.Writing: I.L., A.S., K.V., S.N. Data acquisition: I.L., F.N.H., J.H.C. Dataanalysis: I.L. A.S., S.N. Critical review: I.L., A.S., K.V., S.N., F.N.H., S.P.M.,A.S.L., S.T., M.M., R.J.G., N.S., J.H.C.All authors readandapprovedthe ﬁnalhttps://doi.org/10.1038/s41746-024-01377-1 Article
npj Digital Medicine |            (2025) 8:45 10
manuscript and had ﬁnal responsibility for the decision to submit it for
publication.
Competing interests
A.S. owns stock in Roche (RHHVF) and Cerebral Inc. and is an adviser toDaybreak Health and Cerebral Inc. S.N. owns stock in Meta, works at Insitro(an ML for drug discovery company), and owns stock options for Insitro.N.H.S. reported being a co-founder of Prealize Health (a predictive analyticscompany) and Atropos Health (an on-demand evidence generation com-pany); receiving funding from the Gordon and Betty Moore Foundation fordeveloping virtual model deployments; and serving on the Board of theCoalition for Healthcare AI (CHAI), a consensus-building organization pro-viding guidelines for the responsible use of arti ﬁcial intelligence in health-
care. J.H.C.reportedbeing a co-founderof Reaction Explorer LLC, developsand licenses organic chemistry education software, paid consulting fees
from Sutton Pierce, Younker Hyde MacFarlane, and Sykes McAllister as a
medical expert witness, and paid consulting fees from ISHI Health. Theremaining authors declare no competing interests.
Additional information
Supplementary information The online version contains
supplementary material available athttps://doi.org/10.1038/s41746-024-01377-1
.Correspondence and requests for materials should be addressed to
Ivan Lopez.
Reprints and permissions information is available at
http://www.nature.com/reprints
Publisher ’s note Springer Nature remains neutral with regard to jurisdictional
claims in published maps and institutional af ﬁliations.
Open Access This article is licensed under a Creative Commons
Attribution 4.0 International License, which permits use, sharing,adaptation, distribution and reproduction in any medium or format, as longas you give appropriate credit to the original author(s) and the source,provide a link to the Creative Commons licence, and indicate if changeswere made. The images or other third party material in this article areincluded in the article ’s Creative Commons licence, unless indicated
otherwise in a credit line to the material. If material is not included in the
article ’s Creative Commons licence and your intended use is not permitted
by statutory regulation or exceeds the permitted use, you will need toobtain permission directly from the copyright holder. To view a copy of thislicence, visit http://creativecommons.org/licenses/by/4.0/
.
© The Author(s) 2025https://doi.org/10.1038/s41746-024-01377-1 Article
npj Digital Medicine |            (2025) 8:45 11